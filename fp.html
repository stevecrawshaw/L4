<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Steve Crawshaw">
<meta name="dcterms.date" content="2023-04-26">

<title>Diagnostics and Quality Reporting Analytics Pipeline for Air Quality Monitoring</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="fp_files/libs/clipboard/clipboard.min.js"></script>
<script src="fp_files/libs/quarto-html/quarto.js"></script>
<script src="fp_files/libs/quarto-html/popper.min.js"></script>
<script src="fp_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="fp_files/libs/quarto-html/anchor.min.js"></script>
<link href="fp_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="fp_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="fp_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="fp_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="fp_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#project-scope" id="toc-project-scope" class="nav-link" data-scroll-target="#project-scope"><span class="toc-section-number">2</span>  Project Scope</a></li>
  <li><a href="#project-plan" id="toc-project-plan" class="nav-link" data-scroll-target="#project-plan"><span class="toc-section-number">3</span>  Project Plan</a></li>
  <li><a href="#key-performance-indicators" id="toc-key-performance-indicators" class="nav-link" data-scroll-target="#key-performance-indicators"><span class="toc-section-number">4</span>  Key Performance Indicators</a></li>
  <li><a href="#overview-of-regulatory-regime-and-business-context" id="toc-overview-of-regulatory-regime-and-business-context" class="nav-link" data-scroll-target="#overview-of-regulatory-regime-and-business-context"><span class="toc-section-number">5</span>  Overview of Regulatory Regime and Business Context</a>
  <ul class="collapse">
  <li><a href="#regulatory-regime" id="toc-regulatory-regime" class="nav-link" data-scroll-target="#regulatory-regime"><span class="toc-section-number">5.1</span>  Regulatory Regime</a></li>
  <li><a href="#monitoring-network-and-calibrations" id="toc-monitoring-network-and-calibrations" class="nav-link" data-scroll-target="#monitoring-network-and-calibrations"><span class="toc-section-number">5.2</span>  Monitoring Network and Calibrations</a></li>
  </ul></li>
  <li><a href="#data-sources" id="toc-data-sources" class="nav-link" data-scroll-target="#data-sources"><span class="toc-section-number">6</span>  Data Sources</a></li>
  <li><a href="#data-classification" id="toc-data-classification" class="nav-link" data-scroll-target="#data-classification"><span class="toc-section-number">7</span>  Data Classification</a></li>
  <li><a href="#data-processing-pipeline" id="toc-data-processing-pipeline" class="nav-link" data-scroll-target="#data-processing-pipeline"><span class="toc-section-number">8</span>  Data Processing Pipeline</a></li>
  <li><a href="#data-processing-and-analysis-examples" id="toc-data-processing-and-analysis-examples" class="nav-link" data-scroll-target="#data-processing-and-analysis-examples"><span class="toc-section-number">9</span>  Data Processing and Analysis Examples</a>
  <ul class="collapse">
  <li><a href="#derive-summary-statistics-for-missing-data" id="toc-derive-summary-statistics-for-missing-data" class="nav-link" data-scroll-target="#derive-summary-statistics-for-missing-data"><span class="toc-section-number">9.1</span>  Derive Summary Statistics for Missing Data</a></li>
  <li><a href="#extract-and-process-diagnostics-data" id="toc-extract-and-process-diagnostics-data" class="nav-link" data-scroll-target="#extract-and-process-diagnostics-data"><span class="toc-section-number">9.2</span>  Extract and Process Diagnostics Data</a></li>
  <li><a href="#extract-and-process-calibration-data" id="toc-extract-and-process-calibration-data" class="nav-link" data-scroll-target="#extract-and-process-calibration-data"><span class="toc-section-number">9.3</span>  Extract and Process Calibration Data</a></li>
  <li><a href="#create-output-table-for-calibration-factors-and-targets" id="toc-create-output-table-for-calibration-factors-and-targets" class="nav-link" data-scroll-target="#create-output-table-for-calibration-factors-and-targets"><span class="toc-section-number">9.4</span>  Create Output Table for Calibration Factors and Targets</a></li>
  <li><a href="#retrieving-airtime-data-from-routers" id="toc-retrieving-airtime-data-from-routers" class="nav-link" data-scroll-target="#retrieving-airtime-data-from-routers"><span class="toc-section-number">9.5</span>  Retrieving Airtime Data from Routers</a></li>
  <li><a href="#plot-daily-data-use-of-routers" id="toc-plot-daily-data-use-of-routers" class="nav-link" data-scroll-target="#plot-daily-data-use-of-routers"><span class="toc-section-number">9.6</span>  Plot Daily Data use of Routers</a></li>
  </ul></li>
  <li><a href="#targets-pipeline" id="toc-targets-pipeline" class="nav-link" data-scroll-target="#targets-pipeline"><span class="toc-section-number">10</span>  Targets Pipeline</a></li>
  <li><a href="#project-outcome-reporting-product" id="toc-project-outcome-reporting-product" class="nav-link" data-scroll-target="#project-outcome-reporting-product"><span class="toc-section-number">11</span>  Project Outcome: Reporting Product</a></li>
  <li><a href="#generating-the-qa-report" id="toc-generating-the-qa-report" class="nav-link" data-scroll-target="#generating-the-qa-report"><span class="toc-section-number">12</span>  Generating the QA Report</a></li>
  <li><a href="#research-and-findings" id="toc-research-and-findings" class="nav-link" data-scroll-target="#research-and-findings"><span class="toc-section-number">13</span>  Research and Findings</a></li>
  <li><a href="#summary-of-project-activities" id="toc-summary-of-project-activities" class="nav-link" data-scroll-target="#summary-of-project-activities"><span class="toc-section-number">14</span>  Summary of Project Activities</a></li>
  <li><a href="#recommendations-and-conclusions" id="toc-recommendations-and-conclusions" class="nav-link" data-scroll-target="#recommendations-and-conclusions"><span class="toc-section-number">15</span>  Recommendations and Conclusions</a>
  <ul class="collapse">
  <li><a href="#business-benefits" id="toc-business-benefits" class="nav-link" data-scroll-target="#business-benefits"><span class="toc-section-number">15.1</span>  Business Benefits</a></li>
  </ul></li>
  <li><a href="#appendices" id="toc-appendices" class="nav-link" data-scroll-target="#appendices"><span class="toc-section-number">16</span>  Appendices</a>
  <ul class="collapse">
  <li><a href="#appendix-1-ksb-mapping" id="toc-appendix-1-ksb-mapping" class="nav-link" data-scroll-target="#appendix-1-ksb-mapping"><span class="toc-section-number">16.1</span>  Appendix 1: KSB Mapping</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Diagnostics and Quality Reporting Analytics Pipeline for Air Quality Monitoring</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Steve Crawshaw </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 26, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The aim of this project is to create a Reproducible Analytic Pipeline (RAP) to collate, summarise and report the operating characteristics of the council’s air quality monitoring network and provide assurance that the system is operating within desired parameters. This minimises data quality risks by identifying problems and mitigations in a timely, structured and evidenced way.</p>
<p>The project complies with relevant guidance from Defra who are the ultimate customer for this activity.</p>
<p>Apprenticeship KSBs are logged in the text in square parentheses in bold e.g.&nbsp;<strong>[S1, K11]</strong></p>
</section>
<section id="project-scope" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Project Scope</h1>
<p>The scope of this project is the operation and calibration of the continuous air monitors operated by Bristol City Council. The council operates other monitors (diffusion tubes), but these will not be covered in this project as the quality review period is annual rather than monthly and different data collection and quality criteria apply. Diffusion tube data quality is largely captured in another portfolio project.</p>
<p>The diagnostics element of this project only applies to the NOx (Oxides of Nitrogen) instruments. The council operates PM<sub>2.5</sub> instruments, but diagnostics data are not recorded in a usable way.</p>
<p>Summary analysis of data loss is included as there are minimum data capture requirements to meet according to the technical guidance. Performance and quality metrics for telemetry equipment associated with the continuous analysers are reported. <strong>[K8, S6]</strong></p>
<p>Summaries of calibration data quality are included in scope.</p>
</section>
<section id="project-plan" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Project Plan</h1>
<ul>
<li>Review relevant guidance and consult with colleagues to identify appropriate report outputs <strong>[K9, S7]</strong></li>
<li>Identify and confirm data sources as inputs to the project</li>
<li>Confirm optimal approach to building a robust RAP <strong>[B4]</strong></li>
<li>Develop code for pipeline</li>
<li>Test code to verify accurate reporting</li>
<li>Liaise with colleagues to confirm acceptability <strong>[K9, S7]</strong></li>
<li>Adjust pipeline as necessary</li>
<li>Move to production</li>
</ul>
</section>
<section id="key-performance-indicators" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Key Performance Indicators</h1>
<p>This project is about reporting KPIs which relate to the operation of the QA process for air quality data. The KPIs of interest are:</p>
<ul>
<li>Data collection better than 85% for hourly data reported annually</li>
<li>4G data use below 3GB per site across router fleet per month</li>
<li>Identification of gas contamination within 3 weeks of last calibration</li>
<li>Identification of analyser system faults</li>
</ul>
<p>In terms of the project itself, the KPI is that colleagues must be able to run the monthly report and review results, taking appropriate action where needed. This was established through dialogue with colleagues. <strong>[S12, B3]</strong></p>
</section>
<section id="overview-of-regulatory-regime-and-business-context" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Overview of Regulatory Regime and Business Context</h1>
<section id="regulatory-regime" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="regulatory-regime"><span class="header-section-number">5.1</span> Regulatory Regime</h2>
<p>It is important to set the context of air quality monitoring in order to understand the regulatory framework, monitoring requirements and implications of low - quality data and hence analysis presented in the project.</p>
<p>Under The Environment Act (1995) local authorities must monitor air quality. If pollutant concentrations exceed limits, an Air Quality Management Area must be declared, and an air quality action plan implemented. This is called the Local Air Quality Management regime. In addition to this system, the UK government published its <a href="https://www.gov.uk/government/publications/air-quality-plan-for-nitrogen-dioxide-no2-in-uk-2017">Air Quality Plan for nitrogen dioxide</a> in 2017.</p>
<p>This has resulted in certain local authorities (including Bristol) being required to introduce specific “<a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/911404/air-quality-direction-bristol-city-council-no2.pdf">local plans</a>” to deliver compliance with legal limits for nitrogen dioxide (NO<sub>2</sub>). In Bristol the key measure in the local plan is a Clean Air Zone (CAZ), which charges polluting vehicles to enter a central zone in order to encourage faster turnover in the fleet.</p>
<p>The CAZ was approved after submission of a Final Business Case (FBC). The <a href="https://democracy.bristol.gov.uk/documents/s57350/FBC-07%20BCC%20CAZ%20FBC%20Financial%20Case%2017%20Feb%202021.pdf">FBC</a> estimated a total capital cost of £44.3 million for the CAZ.</p>
<p>The FBC included dispersion modelling of air quality to include the predicted compliance date for meeting the NO<sub>2</sub> limit values. The modelling is validated by measurements from our monitoring network. Hence the business case for the CAZ relies on the quality of the data captured by the continuous air monitoring network. This project aims to mitigate the risks of poor data quality and protect the council’s reputation and financial base. <strong>[K1, B1, K8, S6]</strong></p>
</section>
<section id="monitoring-network-and-calibrations" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="monitoring-network-and-calibrations"><span class="header-section-number">5.2</span> Monitoring Network and Calibrations</h2>
<p>The continuous monitoring sites deployed in Bristol are shown in the map below. <img src="images/contin_sites_sml.png" class="img-fluid" alt="Map of continuous monitoring sites"></p>
<p>The technical and data quality requirements for air monitoring of NOx with continuous analysers is contained in pages 127 to 131 of <a href="https://laqm.defra.gov.uk/wp-content/uploads/2022/08/LAQM-TG22-August-22-v1.0.pdf">LAQM.TG(22)</a>. In brief, each site is visited fortnightly, and the NOx instruments are calibrated. <strong>[K1, B1]</strong></p>
<p>Calibration is required because the instruments operate on a chemiluminescence principle where the concentration of the gas is determined by an optical measurement. The optical sensor’s response can diminish with time and contamination and hence the response of the instrument needs to be periodically determined. Calibration entails applying a known zero and span signal to the instrument fortnightly. This is attained by connecting a “clean” (zero) air supply via a scrubber which filters out any contaminant gas. Secondly a traceable gas is connected and the instrument’s span response recorded. A sample calibration procedure is reproduced below for a single site. <strong>[K8, S6]</strong></p>
<div style="page-break-after: always;"></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/lso_proc.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">LSO Calibration procedure</figcaption><p></p>
</figure>
</div>
<p>Zero and span calibration factors are calculated from the form data. Instead of adjusting the response of the machine to these calibrations, the data themselves are changed manually by applying a linear correction in the database. Spurious or anomalous data are also removed. Every six months, the instruments are fully serviced and their responses are corrected to the calibration standards. <strong>[K8, S6]</strong></p>
</section>
</section>
<section id="data-sources" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Data Sources</h1>
<p>The sources of data are summarised as follows: <strong>[S4]</strong></p>
<ol type="1">
<li>SQL server database (Envista). Stores air quality and diagnostics data. This is the only data source which is defined as organisational. The others are third party cloud providers. <strong>[S1]</strong></li>
<li>Google Drive. Calibration data is collected in the field with Google forms and stored in Google sheets.</li>
<li>Teltonika Remote Management System (RMS). 4G router parameters through a REST API.</li>
</ol>
</section>
<section id="data-classification" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Data Classification</h1>
<p>The data used are not classified as personal data. They relate to the operation of the monitoring network and are only of interest to the team managing the operation of the network, or a third party auditor of air quality data. The data are not published but could be made available on request. The data are structured, tabular or converted to such from a REST API source. <strong>[K4, S3]</strong></p>
</section>
<section id="data-processing-pipeline" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Data Processing Pipeline</h1>
<p>The pipeline for this project was built in R, using the <code>targets</code> package to ensure reproducibility and organise the functions. Data are extracted using database connections, the <code>googlesheets4</code> package and the RMS API. Various data cleaning and processing functions are implemented in the pipeline to prepare the data for output in a <a href="https://quarto.org/">Quarto</a> document. <strong>[K3, S2]</strong></p>
</section>
<section id="data-processing-and-analysis-examples" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Data Processing and Analysis Examples</h1>
<p>Key elements of the analysis code are reproduced in this section. All the code is available on <a href="https://github.com/stevecrawshaw/L4/tree/main/fp">GitHub</a>.</p>
<section id="derive-summary-statistics-for-missing-data" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="derive-summary-statistics-for-missing-data"><span class="header-section-number">9.1</span> Derive Summary Statistics for Missing Data</h2>
<p>A key requirement within the air quality guidance is that data capture should match or exceed 85% for hourly continuous measurements. If significant data are missing it indicates a machine or telemetry fault. This would result in a service engineer call out. <strong>[K8 S6]</strong></p>
<p>To calculate this statistic, the hourly continuous data for each instrument are retrieved from the database and a function is iterated over the complete dataset to derive the number and percentage of missing observations. The function to retrieve data is not shown for brevity.</p>
<p>The consolidated <code>long_aq_data_tbl</code> is split into a list of tibbles (data frames), completely empty columns removed, and the <code>miss_var_summary()</code> function from the <code>naniar</code> package is mapped over each tibble in the list. The relevant pollutants are filtered from the resulting tibble and the columns are selected and renamed with <code>transmute()</code> function. The resulting tibble is piped into a <code>gt()</code> table. <strong>[K3, S2, S7]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-calc-missing.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Missing data calculation function</figcaption><p></p>
</figure>
</div>
</section>
<section id="extract-and-process-diagnostics-data" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="extract-and-process-diagnostics-data"><span class="header-section-number">9.2</span> Extract and Process Diagnostics Data</h2>
<p>Diagnostics data from the NOx analysers are captured in a database on a data logger. Loggers are polled hourly and both measurement and diagnostics data are stored in a corporate database. The diagnostics data are extracted for the reporting. <strong>[S8]</strong></p>
<p>The code below sets a database connection by retrieving the connection parameters from a config.yml file. The config file is added to .gitignore to ensure that credentials are not published on GitHub. <strong>[S1]</strong></p>
<p>The connection object is used to retrieve the diagnostics table in the <code>get.diag.tbl()</code> function. This function also takes date parameters and filters the diagnostics table so that the temporal subset of data needed is retrieved. This function uses the <code>dbplyr</code> back end to generate SQL in R. <strong>[K11, S15]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-diag-retrieve.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Extract and process diagnostics table from database</figcaption><p></p>
</figure>
</div>
<p>The function below takes a long version of the diagnostics tibble, a tibble holding upper and lower limits for the diagnostics parameters, and a tibble relating the site ID’s in the diagnostics table to meaningful site names. It filters for missing, cleans column data by trimming white space and joins the three data sets to return a cleaned tibble in long format used in plotting and tabulation. The upper and lower limit are used to define the normal operating characteristics of the instrument. <strong>[S8]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-join-filter-mutate-diag-table.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Cleaning and joining diagnostics data</figcaption><p></p>
</figure>
</div>
</section>
<section id="extract-and-process-calibration-data" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="extract-and-process-calibration-data"><span class="header-section-number">9.3</span> Extract and Process Calibration Data</h2>
<p>The calibration data are entered on a mobile device using Google forms and are held on a Google spreadsheet which calculates the zero and span factors from the data entered. Google sheets was used as it runs on any mobile device and data are not personal or private. An exemption to a corporate ban on Google forms was secured for this use case. <strong>[S1, K12]</strong> The screenshot below shows a portion of the calibration form used on a mobile device.</p>
<div style="page-break-after: always;"></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/form_cals.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Calibration data capture form</figcaption><p></p>
</figure>
</div>
<p>The <code>googlesheets4</code> package is used to access the spreadsheet using the Google credentials object. The two functions shown below retrieve calibration and gas metadata from the spreadsheet and range specified in the <code>read_sheet()</code> functions and perform minimal data cleaning before returning a tibble. <strong>[S8, S4]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-get-cal-gas-tbl.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Retrieve calibration and gas data</figcaption><p></p>
</figure>
</div>
</section>
<section id="create-output-table-for-calibration-factors-and-targets" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="create-output-table-for-calibration-factors-and-targets"><span class="header-section-number">9.4</span> Create Output Table for Calibration Factors and Targets</h2>
<p>The <code>gt</code> package was used to create a table showing how calibrations compare against optimal values. The data retrieved is pre - processed and passed to the function below. This function filters the data, sets targets for the calibration factors - zero for zero calibration and one for span calibration and groups the resulting data frame by site and date. This tibble is then piped into the <code>gt()</code> function which creates the output table as html. Formatting functions render the table in a way which conveys the summary quickly and clearly to the reader. <strong>[S12]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-cal-factors-gt.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Calibration factors table</figcaption><p></p>
</figure>
</div>
</section>
<section id="retrieving-airtime-data-from-routers" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="retrieving-airtime-data-from-routers"><span class="header-section-number">9.5</span> Retrieving Airtime Data from Routers</h2>
<p>I specified the Teltonika routers and RMS because RMS enables secure remote management of the fleet of routers. RMS also supports remote access to devices connected to the routers, such as the windows data loggers which collect and store data from the analysers. The operation of the instruments, loggers and routers can be checked with RMS. This means site visits can be minimised. <strong>[K12, K11, S15]</strong> In addition to the web platform, there is a REST API which is used to retrieve router meta - data including airtime data use.</p>
<p>Airtime data is limited by contract to 3GB per device spread over all SIMs. Exceeding this limit results in excess charges or telemetry loss, hence it is important to regularly review usage. The screenshot below shows the device overview for the routers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/rms_screen.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">RMS screenshot</figcaption><p></p>
</figure>
</div>
<p>The function shown below takes the date span of interest, the device_url endpoint, the API Personal Access Token, and a data frame holding the device ID’s. The <code>single.site.data()</code> function gets data for a single device. This function is then partialised and mapped over the ID’s of the routers to return a data frame with the daily data use for each device in a single table. This is subsequently used for plotting. <strong>[S8, S4]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-data-use-routers.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Retrieve airtime data from routers</figcaption><p></p>
</figure>
</div>
</section>
<section id="plot-daily-data-use-of-routers" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="plot-daily-data-use-of-routers"><span class="header-section-number">9.6</span> Plot Daily Data use of Routers</h2>
<p>The function shown below takes a tibble and uses the <code>ggplot2</code> package to plot a faceted column chart showing daily receive and transmit data totals for each router. Appropriate theme adjustments are made to make a visually appealing chart. <strong>[S12]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-daily-data-plot-routers.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Daily data use</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="targets-pipeline" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Targets Pipeline</h1>
<p>All the functions used in the pipeline are implemented through the <code>targets</code> package. This creates a reproducible workflow by learning the dependencies in the pipeline and skipping redundant targets. This saves time when designing and testing the pipeline as computationally expensive targets are not re - run unnecessarily. Objects (targets) created by the targets pipeline are hashed to track changes to the data file. The hashes determine whether the target needs to be re - run. <strong>[K8, K11, B4]</strong></p>
<p>The targets pipeline is built by coding a list of targets, where each target’s name is the object resulting from the command (function) of that target. A truncated extract of the <code>targets</code> list is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-targets-extract.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Truncated targets pipeline list</figcaption><p></p>
</figure>
</div>
<p>The image below is a subset of the entire network graph for the targets pipeline in this project and shows how functions, targets and outputs are related. The dependencies for the <code>data_summary_tbl</code> object are highlighted.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/tar-visnetwork.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Extract of targets pipeline visual network graph</figcaption><p></p>
</figure>
</div>
</section>
<section id="project-outcome-reporting-product" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Project Outcome: Reporting Product</h1>
<p>Colleagues indicated that the final reporting product needs to be easily readable and to highlight the key parameters which govern measurement quality and system reliability <strong>[S7, K9, S12]</strong>. I decided that an html output would be the ideal option <strong>[B4]</strong>. The file is portable across systems and can be easily published for sharing with engineers on a website such as <a href="https://quartopub.com/">Quarto pub</a>.</p>
<p>The key outputs in the report are as follows.</p>
<ol type="1">
<li>A <code>gt</code> table showing the amount and percentage of missing data for the measurement period and cumulatively from year start.</li>
<li>A <code>gt</code> table with spark lines showing the calibration factors used and their ideal target values. This is broken down by pollutant and site and indicates whether calibrations are providing correct data which which to adjust the measurement data.</li>
<li>A chart showing the divergence of span values for two pollutants. This can indicate problems with contamination in calibration gas cylinders.</li>
<li>Charts showing the data use of airtime data by routers. This can identify excessive data use, which could indicate security breach or operating system problem. If data thresholds are exceeded, airtime can be cut which would inhibit our real time reporting of air quality measurements.</li>
<li>Time series charts of instrument diagnostics. These are compared to high and low “normal operating characteristics” to show when instrument problems may be developing. This could for example be the sample pressure declining, indicating a leak in the system.</li>
</ol>
</section>
<section id="generating-the-qa-report" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Generating the QA Report</h1>
<p>The report components listed above are generated by the targets pipeline and compiled into a Quarto document rendered into html. This process works by referencing the objects created in the targets pipeline within R code chunks in the <code>qa_report.qmd</code> Quarto document. The document is rendered by the pipeline call to <code>tarchetypes::tar_render()</code>. <strong>[K3, S2]</strong></p>
<p>An extract from the QA report is reproduced below to illustrate the format.</p>
<p>The top three lines are YAML which sets up the title, author and format. The next chunk, denoted by back ticks (```) sets options and packages. The following sections are combined text and code chunks. The R code in the chunks reads the relevant target to source and render the visualisation using the <code>tar_read()</code> function.</p>
<p>The overall process for running the pipeline and rendering the report is in two steps.</p>
<ol type="1">
<li>Enter the start and end dates in the functions.R file</li>
<li>Run _targets.R using the <code>tar_make()</code> command</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/r-code-quarto-qa-report.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Extract of Quarto QA report</figcaption><p></p>
</figure>
</div>
</section>
<section id="research-and-findings" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Research and Findings</h1>
<p>Some examples of visualisations are shown below.</p>
<p>This chart shows the diagnostics data for a single site. Some familiarity with the principle of operation of the instruments is needed to parse the data shown. The officer reading the report can be guided by the upper and lower limits shown in dotted blue lines. These limits are defined by reference to the manual for the <a href="https://www.teledyne-api.com/prod/Downloads/06858F%20-%20MANUAL%20OPERATORS%20T200.pdf">Teledyne T200 instruments</a>. The chart or document can easily be sent to the contractors to help them diagnose problems with the instrument.</p>
<p>The instruments contain components that should be maintained within certain parameters, for example Internal Box Temperature which should not go above 40˚C. Similarly a sample flow rate that deviates from the limits could indicate a leak or pump failure. <strong>[S12]</strong></p>
<div style="page-break-after: always;"></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/diag_plot_1-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Diagnostics plot: Brislington</figcaption><p></p>
</figure>
</div>
<p>The bar chart shown below indicates the daily receive and transmit data use for each router. Six sites operated by BCC have loggers running Windows 10. Windows updates and other downloads related to the loggers consume the majority of the data. Updates are often run at the end of the month so high data use is normal to see here.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/data_use_1-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Daily 4G data use</figcaption><p></p>
</figure>
</div>
<p>The chart below uses the same data but accumulates data usage. Here it can be seen that one site (Wells Road) is using an unusually large amount of data. This was investigated and the cause (windows update settings) rectified. <strong>[B4]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cumulative_data_use-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Cumulative data use for period</figcaption><p></p>
</figure>
</div>
<p>The calibration factors chart shown below gives a visual check for the zero and span calibration factors for each pollutant at each calibration.</p>
<p>Zero calibration factors should be close to zero. When levels approach 2 or greater, this could indicate a leak in the sample line to the scrubber.</p>
<p>Sensitivity calibration factors should be close to 1. Factors greater than 1.5 could indicate gas contamination. <strong>[S12]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cal_factors.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Calibration factors</figcaption><p></p>
</figure>
</div>
<p>Span readings should be similar for NOx and NO. The stated value for NOx and NO on the trace gas is usually within 1 - 2 ppb of each other. High variance in span calibration readings can indicate gas contamination.</p>
<p>In the example shown below, the Parson Street site is showing a significant change in the divergence between NOx and NO span values. This should trigger a call out to the engineers. In this case the cause is likely to be contaminated gas, where air has entered the cylinder and changed the concentration of the gas through oxidation. <strong>[K8, S6]</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/span_diff_plot.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Span divergence</figcaption><p></p>
</figure>
</div>
</section>
<section id="summary-of-project-activities" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> Summary of Project Activities</h1>
<ol type="1">
<li>Review of technical guidance and liaison with colleagues to confirm general approach and desired output <strong>[S7, K9]</strong></li>
<li>Identify and configure data sources to be used in the project <strong>[S1]</strong>
<ol type="a">
<li>Procure and configure Teltonika RMS and access via API with PAT <strong>[K12]</strong></li>
<li>Design and configure field data collection system with tablet and Google forms (before project) <strong>[K12]</strong></li>
<li>Configure analysers and data loggers to support automatic collection of diagnostics data (before project) <strong>[B3]</strong></li>
</ol></li>
<li>Research and test pipeline tools for analysis <strong>[K3, S2]</strong></li>
<li>Write and test code in R to deliver pipeline in <code>targets</code> <strong>[K3, S2]</strong></li>
<li>Write Quarto reporting document</li>
<li>Present to colleagues to confirm acceptability of concept <strong>[S7, S12]</strong></li>
<li>Move to production <strong>[K3, S2]</strong></li>
</ol>
</section>
<section id="recommendations-and-conclusions" class="level1" data-number="15">
<h1 data-number="15"><span class="header-section-number">15</span> Recommendations and Conclusions</h1>
<p>This project implements an important set of data quality controls related to continuous air monitoring and contributes towards greater confidence in the air quality monitoring. I have gained a greater understanding of the operation of the entire system through undertaking this work and feel able to fully explain its operation to stakeholders.</p>
<p>Implementing the project through <code>targets</code> and a <code>Quarto</code> report is helpful to end users as it automates the process. However there is a risk that code may break, because of updates to libraries or unanticipated data. This could be mitigated by introducing a reproducible environment management component like <code>renv</code> or <code>anaconda</code>. Alternatively, a docker image could be built to control both libraries and the version of R used. Support for these options within the organisation is limited and relies on personal accounts and access to virtual machines. Further unit testing of the code would improve reliability. <strong>[K8, B11, B4]</strong></p>
<p>Ensuring the long term viability of this approach means that the end user needs to have an understanding of R and be able to parse the code written. This relies on in house expertise in R, and although colleagues are learning R, they may not be able to resolve issues in the short term if I am absent. Nonetheless, the code base will be there as a resource for colleagues when capability has been built.</p>
<p>There is more that could be done to add to this report. For example, because the calibration factors are manually applied to the data in the database client application, there is potential for human error. This could be checked by running routines to verify correct calibration factors are applied. Alternatively the factors could be automatically applied with update queries on the database. However, there is still a manual element to data ratification in terms of removing anomalies so some potential human error would remain. <strong>[K8, S6, K11]</strong></p>
<p>The analytics pipeline could be further developed by creating an R package. This would be the optimal way to create a coherent set of processing and analytic functions. This is something I am researching but will not be able to implement in this project. <strong>[K11]</strong></p>
<section id="business-benefits" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="business-benefits"><span class="header-section-number">15.1</span> Business Benefits</h2>
<p>It is vital that stakeholders have confidence in the quality of air quality data. Multi - million pound decisions are taken on the evidence from air monitoring, such as the implementation of a Clean Air Zone. There is therefore a great deal of scrutiny of the data. The output from this project demonstrates that a rigorous and transparent QA process is conducted at monthly intervals on the data. It also ensures that operational parameters are regularly reviewed to maintain reliable data flow and real time publishing of our air quality data. <strong>[K8, S6]</strong></p>
</section>
</section>
<section id="appendices" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> Appendices</h1>
<section id="appendix-1-ksb-mapping" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="appendix-1-ksb-mapping"><span class="header-section-number">16.1</span> Appendix 1: KSB Mapping</h2>
<div class="cell">

</div>
<p><strong>K3: Principles of the data life cycle and the steps involved in carrying out routine data analysis tasks</strong></p>
<p>The reporting pipeline takes data stored in databases, online spreadsheets and remote monitoring platform and conducts data analysis steps to assess data quality and provide summaries and visualisations for end users.</p>
<p><strong>K4: Principles of data including open and public data administrative data and research data</strong></p>
<p>The data used are not personal data. They relate to the operation of the monitoring network and are only of interest to the team managing the operation of the network, or a third party auditor of air quality data. The data are not published but could be made available on request.</p>
<p><strong>K8: Quality risks inherent in data and how to mitigate or resolve these</strong></p>
<p>This project is about mitigating data quality risks for air monitoring data. It seeks to minimise risks arising from the calibration of the instruments and the operation of the system by tracking and reporting key parameters.</p>
<p><strong>K9: Principal approaches to defining customer requirements for data analysis</strong></p>
<p>I have consulted with colleagues about the format of the report and the metrics to be included in the analysis and adapted the reporting pipeline to deliver this. I have presented the work to my team and trained them to run the pipeline.</p>
<p><strong>K11: Approaches to organisational tools and methods for data analysis</strong></p>
<p>I have used the R software to process and analyse the data, developing secure and robust analyses in line with corporate requirements.</p>
<p><strong>K12: Organisational data architecture</strong></p>
<p>I have used the relevant data sources such as the SQL Server database “Envista” to source air quality and diagnostics data. This is an Azure database maintained corporately. I have used other approved data sources such as Google spreadsheet and RMS to source data.</p>
<p><strong>S1: Use data systems securely to meet requirements and in line with organisational procedures and legislation including principles of Privacy by Design</strong></p>
<p>In accessing and processing the data I have followed best practice to obscure credentials, by using the <code>config</code> package. None of the data used are personal. There are no privacy implications in the analysis conducted and the activities are not covered by GDPR or privacy legislation.</p>
<p><strong>S2: Implement the stages of the data analysis lifecycle</strong></p>
<p>The project report clearly documents the data analysis life cycle. For this work, the reporting is conducted monthly to analyse the previous month’s data and report on quality aspects. Analysis includes extracting, cleaning, combining (joining), summarising, visualising and reporting. No modelling or machine learning is done in this process. Diagnostics and air quality data are retained indefinitely. Telemetry parameters are retained for approximately three months.</p>
<p><strong>S3: Apply principles of data classification within data analysis activity</strong></p>
<p>The data are structured, tabular or converted to such from a REST API source. Unstructured data such as images, video or text corpuses are not used in this work.</p>
<p><strong>S4: Analyse data sets taking account of different data structures and database designs</strong></p>
<p>The project includes multiple examples of analysis of data from different sources (database, API, Google spreadsheet) and structures (tabular, JSON array). Analysis includes functions to summarise data with measures of central tendency, missingness, cumulative sum etc.</p>
<p><strong>S6: Identify and escalate quality risks in data analysis with suggested mitigation or resolutions as appropriate</strong></p>
<p>This project is about data quality risks and mitigations to address this. Examples of how the reporting product addresses quality risks is shown in the cumulative data use chart and the NOx and NO span divergence chart. The risks in the data analysis within this project are covered in the recommendations and conclusions section.</p>
<p><strong>S7: Undertake customer requirements analysis and implement findings in data analytics planning and outputs</strong></p>
<p>I consulted with end users when designing the reporting product and as I developed the pipeline. I received useful feedback and amended the report by including enhanced explanatory text for the calibration charts. To some extent I am the customer for this product too, as I oversee the operation of the monitoring network.</p>
<p><strong>S8: Identify data sources and the risks and challenges to combination within data analysis activity</strong></p>
<p>Much of the analysis within the pipeline requires combining data from different sources. This includes joining by row, with inner or left joins on identifier fields. In addition to these SQL type joins, I use iterative functions like <code>map_dfr()</code> to generate data frames by row when extracting data from multiple tables in the database. It is necessary to ensure column types and names are correct when combining data sources to ensure that data are being joined on the right fields. Selecting the right type of join is important so that the anticipated number of records is returned. New changes in the <code>dplyr</code> join functions help to mitigate the risks in joining data by providing helper functions such as <code>join_by()</code> which I have used in my code.</p>
<p><strong>S12: Collaborate and communicate with a range of internal and external stakeholders using appropriate styles and behaviours to suit the audience</strong></p>
<p>I presented example reports to my colleagues in April 2023 to help improve the analysis and visualisations. The output is not suitable for external stakeholders as it is specifically for internal use. It may be of interest to other local authorities, but this would rely on them having the technical capacity to run R and a similar data architecture, which is unlikely.</p>
<p><strong>S15: Select and apply the most appropriate data tools to achieve the optimum outcome</strong></p>
<p>When considering the the most appropriate tools, the key choice for me was the programming language. The main database was already determined, as was the telemetry platform and field calibration data collection system. My choices about the language were limited to R and python as these are the only two languages I have any knowledge of. I have beginners level knowledge in Python, but intermediate level knowledge in R and I had already developed some functions in R for this type of data analysis that I could adapt to the pipeline. I also felt that developing a reproducible analytic pipeline in R was more achievable because good resources were available to guide me in this process such as the book: <a href="https://raps-with-r.dev/">Building reproducible analytical pipelines with R</a>. Once I had decided to use R, I identified the <code>targets</code> package to develop the pipeline and the <code>quarto</code> package to publish the report. I believe these are appropriate tools and I have satisfied myself that they are fit for this purpose.</p>
<p><strong>B3: Work independently and collaboratively</strong></p>
<p>I have worked on this project mostly on my own, but have consulted colleagues when designing the pipeline and reporting. I am the only person with knowledge of R in my organisation, so have needed to learn independently for the most part.</p>
<p><strong>B4: Logical and analytical</strong></p>
<p>I believe I have shown a logical and analytic approach to problem solving in this project. I have had to think and plan the steps in developing analysis for three separate components of the pipeline and test and integrate them into a functioning and robust analysis. This has helped me develop a workflow for this type of analysis which has advanced my ability in data analytics and added valuable new skills.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>