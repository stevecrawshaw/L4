---
title: "Diagnostics and Quality Reporting Analytics Pipeline for Air Quality Monitoring"
author: "Steve Crawshaw"
format: docx
editor: source
---

# Introduction

The aim of this project is to collate, summarise and report the operating characteristics of the council's air quality monitoring system and provide assurance that the system is operating within desired parameters. The risk to data quality is thereby minimised because data problems are identified early and mitigating measures can be implemented, such as replacing calibration gas, invalidating faulty data, service engineer visits etc. 

The project draws on the relevant guidance from Defra who are the ultimate customer for this activity.


# Project Scope

The scope of this project is the operation and calibration of the continuous air monitors operated by Bristol City Council. There are other air pollution monitors which the council operates (diffusion tubes) but these will not be covered in this project as the quality review period is annual rather than monthly and different data collection and quality criteria apply. Diffusion tube data quality is largely captured in anther portfolio project on annual reporting of air quality.

The diagnostics element of this project only applies to the NOx (Oxides of Nitrogen) instruments operated by the council. The council does operate PM~2.5~ BAM (Beta Attenuation) instruments, but diagnostics information is not recorded in a usable way on the data logging systems, so is not included in this analysis.

In addition to diagnostics, summary analysis will be presented of data loss. This is important as there are minimum data capture requirements to meet according to the technical guidance.

This project also captures performance and quality data for telemetry equipment associated with the continuous analysers.

Summaries of calibration data quality are also included in scope. These relate to the responses of the instruments to traceable zero and span gases and are important to scale the measurement data.

# Overview of the LAQM regime and Monitoring Network

## Regulatory Regime

It is important to set the context of air quality monitoring in order to understand the regulatory framework, monitoring requirements and implications of low quality data and hence analysis presented in the project.

Under The Environment Act (1995) local authorities have a duty to monitor air quality. If regulated pollutants exceed legal limits, an Air Quality Management Area (AQMA) must be declared and an air quality action plan (AQAP) published, consulted on and implemented. This is called the Local Air Quality Management (LAQM) regime. In addition to this system, the UK government published its [Air Quality Plan for nitrogen dioxide in 2017](https://www.gov.uk/government/publications/air-quality-plan-for-nitrogen-dioxide-no2-in-uk-2017). 

This has resulted in certain local authorities (including Bristol) being required to introduce specific "local plans" to deliver compliance with legal limits for nitrogen dioxide (NO~2~) in the shortest time possible. In Bristol the key measure in the local plan is a Clean Air Zone (CAZ), which charges older, polluting vehicles to enter a central zone in order to encourage faster turnover in the fleet than would otherwise occur. Newer vehicles are less polluting.

The design of the CAZ was approved after submission of a Full Business Case (FBC) The [FBC estimated](https://democracy.bristol.gov.uk/documents/s57350/FBC-07%20BCC%20CAZ%20FBC%20Financial%20Case%2017%20Feb%202021.pdf) a total capital cost of Â£44.3 million for the CAZ.

The FBC included scenario testing of different CAZ options and dispersion modelling of air quality to include the predicted compliance date for meeting the NO~2~ limit values. The dispersion modelling is validated by reference to the reported measurements from our air monitoring network. Hence the whole business case for the CAZ relies on the quality of the data captured by the continuous air quality monitoring network for NOx and NO~2~. This project aims to mitigate the risks of poor data quality and protect the council's reputation and financial base.

## Monitoring Network and Calibrations

The continuous monitoring sites deployed in Bristol are shown in the map below.
![Map of continuous monitoring sites](images/contin_sites_sml.png)

The technical and data quality requirements for air monitoring of NOX with continuous analysers is contained in pages [127 to 131 of LAQM.TG(22)](https://laqm.defra.gov.uk/wp-content/uploads/2022/08/LAQM-TG22-August-22-v1.0.pdf). In brief, each site is visited fortnightly and the NOx instruments are calibrated.

Calibration is required because the instruments operate on a chemiluminescence principle. The concentration of the gas is determined by an optical measurement. The optical sensor's response can diminish with time and contamination and hence the response of the instrument needs to be periodically determined. Calibration entails applying a known zero and span signal to the instrument. This is delivered by plumbing in first a "clean" (zero) air supply via a scrubber which filters out any contaminant gas. Secondly a traceable gas sourced from a specialist supplier is connected and the instrument's span response recorded. A sample calibration procedure is reproduced below for a single site.

![LSO Calibration procedure](images/lso_proc.jpeg)

From the zero and span signals obtained from the above operation, zero and span calibration factors are calculated. Instead of adjusting the response of the machine to these calibrations, the data themselves are corrected by a manual process which also removes spurious or anomalous data. Every six months, the instruments are fully serviced and their responses are corrected to the calibration standards.

# Data Sources

The sources of data are summarised as follows:

1.  SQL server database (Envista). Stores air quality measurements and diagnostics data. This is the only data source which is defined as organisational. The others are third party cloud providers.
2.  Google Drive. Calibration data is collected in the field with google forms and stored in google sheets
3.  Teltonika Remote Management System (RMS). Parameters for the 4G routers which provide telemetry to the network are available through a REST API.

# Data Classification

The data used are not classified as personal data. They relate to the operation of the monitoring network and are only of interest to the team managing the operation of the network, or a third party auditor of air quality data such as the UK government or their consultants. The data are not published but could be made available on request. The data are structured, tabular or converted to such from an REST API source.

# Data Processing Pipeline

The pipeline for this project was built in R, using the `targets` package to ensure reproducibility and organise the functions. Data are extracted from the data sources, using database connections, the `googlesheets4` package and the RMS API. Various data cleaning and processing functions are implemented in the pipeline to prepare the data for output in a Quarto document.

# Data Processing Examples

## Derive Summary Statistics for Missing Data

A key requirement within the air quality guidance is that data capture should match or exceed 85% for hourly continuous measurements. Hence a valuable monthly check is the amount of missing data. If significant data are missing it indicates a machine or telemetry fault. This would result in a service engineer call out.

In order to calculate this statistic, the hourly continuous data for each instrument is retrieved from the envista database and a function is iterated over the complete dataset to derive the number and percentage of missing observations. The function to retrieve data from envista is not shown for brevity, but the missing data calculation is shown below.

The consolidated long_aq_data_tbl is split into a list of tibbles, completely empty (redundant) columns removed, and the `miss_var_summary()` function from the `naniar` package is mapped over each tibble in the list. The relevant pollutants are filtered from the resulting tibble and the columns are selected and renamed with `transmute()` function. The resulting tibble is piped into a `gt()` table in the targets pipeline.

![Missing data calculation function](images/r-code-calc-missing.png)


## Extract and Process Diagnostics Data

Diagnostics data from the NOx analysers are captured in a database on an on - site data logger. The  loggers are polled hourly and both measurement and diagnostics data are stored in a corporate database "envista". The diagnostics data are extracted for the reporting.

The code below sets up a database connection by retrieving the connection parameters from a config.yml file. This ensures the credentials are safely stored and not embedded in the code. The config file can be added to .gitignore to ensure that the credentials are not published on GitHub.

The connection object is then used to retrieve the diagnostics table in the `get.diag.tbl()` function. This function also takes date parameters and filters the diagnostics table so that the temporal subset of data needed is retrieved. This function uses the `dbplyr` backend to generate SQL in R.

![Extract and process diagnostics table from database](images/r-code-diag-retrieve.png)

The function below takes a long version of the diagnostics table, a data frame holding upper and lower limits for the diagnostics parameters, and a data frame relating the site ID's in the diagnostics table to meaningful site names. It filters for missing, cleans column data by trimming white space and joins the three data sets to return a cleaned diagnostics table in long format used later in plotting and tabulation. The upper and lower limit data are used in the plotting function to show the normal operating characteristics of the instrument. 

![Cleaning and joining diagnostics data](images/r-code-join-filter-mutate-diag-table.png)

## Extract and Process Calibration Data

The calibration data are entered on a mobile device using Google forms and are held on a Google spreadsheet. The spreadsheet calculates the zero and span factors from the data entered. The screenshot below shows a portion of the calibration form used on a mobile device.

:::{layout-ncol=2 layout-valign="center"}

![Calibration data capture form](images/googleform_cals.png)

![Calibration data capture form](images/googleform_cals_2.png)

:::

The `googlesheets4` package in the `tidyverse` metapackage in R is used to access the spreadsheet using the google credentials object, which is stored in the config.yml file for security. The two functions shown below retrieve calibration and gas metadata from the spreadsheet and range specified in the `read_sheet()` functions and perform minimal data cleaning before returning a data frame.


![Retrieve calibration and gas data](images/r-code-get-cal-gas-tbl.png)

## Create Output Table for Calibration Factors and Targets

The `gt` package in R was used to create a high quality table showing how calibrations compared against ideal values. The calibration data retrieved is pre - processed and passed to the function below. This function filters the data, sets targets for the calibration factors - zero for zero calibration and one for span calibration and groups the resulting data frame by site and date. This data frame is then piped into the `gt()` function which creates the output table as html. Various formatting functions are used to create the table in an attractive format which conveys the necessary information quickly and clearly to the reader.

![Calibration factors table](images/r-code-cal-factors-gt.png)

## Retrieving Airtime Data from Routers

Teltonika's Remote Management System (RMS) is a platform that enables secure remote management of a fleet of Teltonika routers. RMS also supports remote access to devices connected to the routers, such as the windows data loggers that we use to collect and store data from the analysers. This means that we can remotely check the operation of the instruments, the data loggers and the routers using RMS. In addition to the web platform, there is a REST API which is used to retrieve router meta - data including airtime data use. 

Airtime data is limited by contract to 3GB per device spread over all SIMS. Exceeding this limit results in excess charges or potentially loss of telemetry, hence it is important to regularly review usage. The screenshot below shows the device overview for the routers used in the air quality telemetry.

![RMS screenshot](images/rms_screen.png)

The function shown below takes the date span of interest, the device_url endpoint, the API Personal Access Token (PAT), and a data frame holding the device ID's. The `single.site.data()` function within it gets data for a single device. This function is then partialised and mapped over the ID's of the routers to return a data frame with the daily data use for each device in a single table. This is subsequently used in a plot function.

![Retrieve airtime data from routers](images/r-code-data-use-routers.png)

## Plot Daily Data use of Routers

The function shown below takes a data frame and uses the `ggplot` package to plot a faceted column chart showing daily receive and transmit data totals for each router. Appropriate theme adjustments are made to make a visually appealing chart. 

![Daily data use](images/r-code-daily-data-plot-routers.png)

# Targets Pipeline

All the functions used in the pipeline are implemented through the `targets` package. This creates a reproducible workflow by learning the dependencies in the pipeline and skipping redundant targets. This can save time when designing and testing the pipeline as computationally expensive targets are not re - run unnecessarily. Objects (targets) created by the targets pipeline are hashed to track changes to the data file. The hashes determine whether the target needs to be re - run.

The targets pipeline is built by coding a list of targets, where each target's name is the object resulting from the command (function) of that target. A truncated example of the target's list in this project is shown below.

![Truncated targets pipeline list](images/r-code-targets-extract.png)

The dependencies created by the targets pipeline can be viewed using the `tar_visnetwork()` function. The image below is a subset of the entire network graph for the targets pipeline in this project and shows how functions, targets and outputs are related. The dependencies for the `data_summary_tbl` object are highlighted.

![Extract of targets pipeline visual network graph](images/tar-visnetwork.png)

# Reporting

The final reporting product needs to be easily readable and to highlight the key parameters which govern measurement quality and system reliability. I decided that an html output from a Quarto document would be the ideal option. The file is portable across systems and can be easily published for sharing with e.g. service contractors on a website such as Quarto pubs.

The key outputs in the report are as follows.

1.  A `gt` table showing the amount and percentage of missing data for the measurement period
2.  A `gt` table with sparklines showing the calibration factors used and their ideal target values. This is broken down by pollutant and site and indicates whether calibrations are providing correct data which which to adjust the measurement data.
3.  A chart showing the divergence of span values for two pollutants. This can indicate problems with contamination in calibration gas cylinders.
4.  Charts showing the data use of airtime data allowances by routers. This can help identify excessive data use, which could indicate security breach or operating system problem. If data thresholds are exceeded, airtime can be cut which would inhibit our real time reporting of air quality measurements.
5.  Time series charts of instrument diagnostics. These are compared to high and low "normal operating characteristics" to show when instrument problems may be developing. This could for example be the sample pressure declining, indicating a leak in the system.

# Generating the QA Report

The report components listed above are generated by the targets pipeline and compiled into a Quarto document rendered into HTML. This process works by referencing the objects created in the targets pipeline within R code chunks in the `qa_report.qmd` Quarto document. An extract from the QA report is reproduced below to illustrate the format.

The top three lines are YAML which sets up the title, author and format. The next chunk, denoted by backticks (\`\`\`) sets options and packages. The following sections are combined text and code chunks, similar to an iPython notebook. The R code in the chunks reads the relevant target to source and render the visualisation using the `tar_read()` function.

The overall process for running the pipeline and rendering the report is in three steps.

1. Enter the start and end dates in the functions.R file
2. Run _targets.R using the `tar_make()` command
3. Render the qa_report.qmd report to create and display the HTML report

![Extract of Quarto QA report](images/r-code-quarto-qa-report.png)

The final reporting is through an html rendered quarto document. Some examples of visualisations are shown below.

This chart shows the diagnostics data for a single site. Some familiarity with the principle of operation of the instruments is needed to parse the data shown. The officer reading the report can be guided by the upper and lower limits shown in dotted blue lines. These limits are defined by reference to the manual for the [Teledyne T200 instruments](https://www.teledyne-api.com/prod/Downloads/06858F%20-%20MANUAL%20OPERATORS%20T200.pdf).
The chart or document can easily be sent to the equipment support unit (ESU - a third party company that maintains the instruments) to help them diagnose problems with the instrument.

The instruments contain components that should be maintained within certain parameters, for example Internal Box Temperature: the temperature inside the instrument should not go above 40C. Similarly a sample flow rate that deviates from the upper and lower limits could indicate a leak or pump failure.


![Diagnostics plot: Brislington](project-2-diagnostics/qa_report_files/figure-html/diag_plot_1-1.png)

The bar chart shown below indicates the daily receive and transmit data use for each router. Allowances refresh on the first day of each month. Six sites operated by BCC have loggers running Windows 10. Windows updates and other downloads related to the loggers consume the majority of the data. Updates are often run at the end of the month so high data use is normal to see here.

![Daily 4G data use](project-2-diagnostics/qa_report_files/figure-html/data_use_1-1.png)

![Cumulative data use for period](project-2-diagnostics/qa_report_files/figure-html/cumulative_data_use-1.png)

The calibration factors chart shown below gives a visual check for the zero and span calibration factors for each pollutant at each calibration. 

Zero calibration factors should be close to zero. When levels approach 2 or greater, this could indicate a leak in the sample line to the bottle or another problem.

Sensitivity calibration factors should be close to 1. Factors greater than 1.5 could indicate contamination of the gas bottle.

![Calibration factors](images/cal_factors.png)

Span readings should be similar for NOx and NO. The stated value for NOx and NO on the trace gas is usually within 1 - 2 ppb of each other. High variance in span calibration readings can indicate a contaminated cylinder. If no line is plotted it is likely that only one calibration was done in the period selected.

In the example shown below, the Parson Street site is showing a significant change in the divergence between NOx and NO span values. This should trigger a call out to the engineers. In this case the cause is likely to be a contaminated gas cylinder, where air has entered the cylinder and changed the concentration of the gas through oxidation.

![Span divergence](project-2-diagnostics/qa_report_files/figure-html/span_divergence-1.png) 

# Summary of Project Activities 

1. Review of technical guidance and liaison with colleagues to confirm general approach and desired output
2. Identify and configure data sources to be used in the project
    a. Procure and configure Teltonika RMS and access via API with PAT
    b. Design and configure field data collection system with tablet and google forms (prior to              project)
    c. Configure analysers and data loggers to support automatic collection of diagnostics data              (prior to project)
3. Research and test pipeline tools for analysis
4. Write and test code in R to deliver pipeline in `targets`
5. Present to colleagues to confirm acceptability of concept
6. Move to production

# Learning and Reflection

This project implements an important set of data quality controls related to continuous air monitoring and contributes towards greater confidence in the air quality monitoring. I have gained a greater understanding of the operation of the entire system through undertaking this work and feel able to fully explain its operation to stakeholders.

Implementing the project through `targets` and a `Quarto` report is helpful to the officers who will be the customers as it automates the process. However there is a risk that code may break, either because of updates to libraries, or because of unanticipated data. This could be mitigated to some degree by introducing a reproducible environment management component such as `renv` or possibly `anaconda`. Alternatively a docker image could be built to control both libraries and the version of R used. Support for these options within the organisation is limited and relies on personal accounts and access to Virtual Machines. Further unit testing of the code would help too.

Ensuring the long term viability of this approach means that the end user needs to have an understanding of R and be able to parse the code I have written. This is a risk because I am leaving my employer, and although colleagues are learning R, they may not be able to resolve issues in the short term. Nonetheless the code base will be there as a resource for officers when capability has been built.

There is more that could be done to add to this report. For example, because the calibration factors are manually applied to the fortnightly period's data in the database client application, there is potential for human error. This could be checked by running database queries and reporting to verify that the calibration factors applied are correct. Alternatively the calibration factors could be automatically applied with update queries on the database. However there is still a manual element to data ratification in terms of removing anomalies so an element of potential human error would remain.

The analytics pipeline could be further developed by creating an R package. This would be the optimal way to create a coherent set of processing and analytical functions related to this activity. This is something I am researching at the moment but it is unlikely that I can implement in my remaining time.


# Business Benefits

It is vital that stakeholders have confidence in the quality of air quality data. Multi - million pound decisions are taken on the evidence from air quality monitoring such as the implementation of a Clean Air Zone (CAZ). Hence there is a great deal of scrutiny of the data which must be defendable. The output from this project demonstrates that a rigorous QA process is conducted at monthly intervals on the data. It also ensures that operational parameters are regularly reviewed to maintain reliable data flow and real time publishing of our air quality data.
